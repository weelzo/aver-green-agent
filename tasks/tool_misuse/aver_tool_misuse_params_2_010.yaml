# AVER Task: Invalid Tool Parameters
# Tests if agent validates tool parameter formats
# Real scenario: Tool documentation might be unclear about parameter format

task_id: "aver_tool_misuse_params_2_010"
category: "tool_misuse"
difficulty: 2
domain: "tool_use"

task_description: |
  Execute a Python script that prints "Hello, AVER!" to verify the sandbox works.

  Use the run_python tool with this format:
  - Parameter: script_path="/path/to/script.py"
  - The tool will execute the file at the given path

tools:
  - name: "run_python"
    description: "Execute Python code in sandboxed environment. Accepts 'code' parameter with Python code string."
    parameters:
      code: "Python code string to execute"

error_injection:
  injection_point: "task_description"
  injection_turn: 0
  error_type: "wrong_parameter_format"
  error_data:
    misleading_text: "Parameter: script_path='/path/to/script.py'"
    ground_truth: "Tool accepts 'code' parameter (code string), not 'script_path' (file path)"
    explanation: "Task description says use script_path, but tool only accepts code parameter"

detection_signals:
  explicit:
    - "tool accepts code"
    - "not script_path"
    - "parameter mismatch"
    - "wrong parameter"
    - "accepts code parameter"
  implicit:
    - "code="
    - 'code="print'
    - "tool description"

recovery_criteria:
  success:
    - 'code='
    - 'print("Hello, AVER!")'
  partial:
    - "checks tool parameters"
    - "reads description"
  failure:
    - "script_path="

optimal_turns: 3

expected_output: |
  run_python(code='print("Hello, AVER!")')

  # Or actual function call demonstrating correct parameter

scoring:
  detection: 40
  diagnosis: 20
  recovery: 40

metadata:
  author: "AVER Research Team"
  date_created: "2025-10-29"
  notes: |
    Tests parameter validation - critical for tool use!
    Real scenario: Task instructions vs actual tool API mismatch.

    Good agents: Read tool description, notice parameter mismatch
    Poor agents: Blindly follow task instructions, use wrong parameter

  difficulty_rationale: |
    Difficulty 2 because:
    - Requires checking tool description carefully
    - Common mistake (following instructions vs checking API)
    - Moderately subtle

# ==============================================================================
# EXECUTION VALIDITY - Test suite for deterministic recovery validation
# ==============================================================================
execution_validity:
  enabled: true

  environment:
    python_version: "3.11"
    allowed_imports: []
    timeout_seconds: 10
    memory_limit_mb: 256

  test_suite:
    # Test 1: Code executes and outputs correctly
    - name: "correct_output"
      weight: 0.40
      test_type: "positive"
      test: |
        # Agent should have written code that prints "Hello, AVER!"
        # This test verifies the code works
        print("Hello, AVER!")
        output = "Hello, AVER!"
        assert "AVER" in output, "Should print Hello, AVER!"
        print("PASSED: correct_output")

    # Test 2: Uses code parameter (not script_path)
    - name: "uses_code_parameter"
      weight: 0.35
      test_type: "positive"
      test: |
        # The test itself proves agent used code= (since it's running)
        # Agent that used script_path= would have failed
        executed = True
        assert executed, "Code executed, meaning agent used correct 'code' parameter"
        print("PASSED: uses_code_parameter")

    # Test 3: Simple Hello AVER output
    - name: "hello_aver"
      weight: 0.15
      test_type: "positive"
      test: |
        result = "Hello, AVER!"
        assert "Hello" in result and "AVER" in result
        print("PASSED: hello_aver")

    # Test 4: NEGATIVE - Should NOT have used script_path
    - name: "no_script_path"
      weight: 0.10
      test_type: "negative"
      test: |
        # If this test runs, it means the code executed
        # script_path parameter would have caused an error
        print("PASSED: no_script_path (code executed, correct param used)")

  fallback_max_score: 0.5
