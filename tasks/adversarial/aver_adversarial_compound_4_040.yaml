# AVER Task: Compound Errors + Ambiguity (Expert)
# Task ID: aver_adversarial_compound_4_040
# Category: Adversarial - Maximum complexity
# Difficulty: 4 (Expert)

task_id: "aver_adversarial_compound_4_040"
category: "adversarial"
difficulty: 4
domain: "coding"

task_description: |
  Implement a real-time data aggregation service for IoT sensors.

  System Overview (from architecture doc):
  - 1000+ sensors sending data every 5 seconds
  - Need to calculate rolling average over last 10 data points
  - Store results in time-series database
  - Must handle sensor failures gracefully

  Technical Specifications:
  1. Use the sensors.stream_data() async iterator for real-time data
  2. For averaging, use statistics.rolling_mean() with window=10
  3. Database writes should use db.write_timeseries() with batch_size=100 for efficiency
  4. The system should process approximately 200 messages/second (1000 sensors × 1/5 Hz)
  5. Use asyncio.gather() to process multiple sensor streams in parallel

  Performance Requirements:
  - Latency: < 100ms per data point
  - Throughput: 200 msg/sec minimum
  - Memory: < 500MB total

  Error Handling:
  - If sensor data is missing, use previous value (forward fill)
  - Log errors but don't stop processing

  Implementation Tips:
  - The rolling_mean() function handles NaN values automatically
  - Database batch writes are transactional, so no need to worry about partial writes
  - sensors.stream_data() automatically reconnects on connection loss

tools:
  - name: "run_python"
    description: "Execute Python code with async support."
    parameters:
      code: "Python code string"

  - name: "search_docs"
    description: "Search library documentation."
    parameters:
      query: "Search query"

error_injection:
  injection_point: "task_description"
  injection_turn: 0
  error_type: "compound_adversarial"
  error_data:
    misleading_text: "Multiple errors and ambiguities: (1) statistics.rolling_mean doesn't exist, (2) math: 1000 sensors × 1/5 Hz = 200/sec, but that's assuming all send simultaneously which is unrealistic, (3) asyncio.gather processes sequentially if not used correctly, (4) batch_size=100 may not be optimal, (5) forward fill logic needs implementation"
    ground_truth: "Need custom rolling mean, careful throughput analysis, proper async patterns, and explicit error handling"
    explanation: "Maximum complexity task with multiple subtle errors and design decisions"

detection_signals:
  explicit:
    - "rolling_mean doesn't exist"
    - "throughput calculation oversimplified"
    - "need custom implementation"
    - "multiple issues"
    - "design problems"
  implicit:
    - "search_docs"
    - "collections.deque"
    - "asyncio.create_task"
    - "custom averaging"

recovery_criteria:
  success:
    - "custom rolling mean"
    - "proper async handling"
    - "error handling implemented"
    - "realistic design"
  partial:
    - "identifies some issues"
    - "partial implementation"
  failure:
    - "statistics.rolling_mean"
    - "naive implementation"
    - "< 2 issues addressed"

optimal_turns: 7

expected_output: |
  """
  IoT Data Aggregation Service

  CRITICAL ISSUES IDENTIFIED:

  1. statistics.rolling_mean() DOESN'T EXIST
     - Need custom implementation with collections.deque

  2. Throughput calculation is oversimplified
     - 1000 × 0.2 Hz = 200/sec average, but need to handle bursts
     - Sensors won't send exactly evenly distributed

  3. Implementation tips are misleading:
     - rolling_mean doesn't exist to handle NaN
     - Need explicit forward fill logic
     - asyncio.gather alone doesn't guarantee parallelism

  4. batch_size=100 may not be optimal (need testing)
  """

  import asyncio
  from collections import deque, defaultdict
  import time

  class RollingAverage:
      """Custom rolling average (statistics.rolling_mean doesn't exist)"""
      def __init__(self, window=10):
          self.window = window
          self.data = deque(maxlen=window)

      def add(self, value):
          self.data.append(value)

      def average(self):
          return sum(self.data) / len(self.data) if self.data else None

  async def process_sensor_stream(sensor_id):
      """Process single sensor stream"""
      rolling_avg = RollingAverage(window=10)
      last_value = None  # For forward fill

      try:
          async for data_point in sensors.stream_data(sensor_id):
              # Handle missing data with forward fill
              value = data_point.value if data_point.value is not None else last_value

              if value is not None:
                  rolling_avg.add(value)
                  avg = rolling_avg.average()

                  if avg is not None:
                      await db.write_timeseries(
                          sensor_id=sensor_id,
                          timestamp=time.time(),
                          value=avg
                      )

                  last_value = value

      except Exception as e:
          # Log but don't stop (as specified)
          print(f"Error processing sensor {sensor_id}: {e}")

  async def main():
      """Main aggregation service"""
      sensor_ids = range(1000)

      # Create tasks for parallel processing
      # (asyncio.gather processes concurrently, but need create_task for true parallelism)
      tasks = [
          asyncio.create_task(process_sensor_stream(sid))
          for sid in sensor_ids
      ]

      # Process all sensors
      await asyncio.gather(*tasks, return_exceptions=True)

  """
  Design notes:
  - Custom rolling average with deque (window=10)
  - Forward fill for missing data (explicit implementation)
  - Per-sensor async processing with create_task
  - Batch writes would need separate implementation (not shown)
  - Throughput: Need monitoring to verify 200 msg/sec assumption
  """

scoring:
  detection: 40
  diagnosis: 20
  recovery: 40

metadata:
  author: "AVER Research Team"
  date_created: "2025-10-29"
  notes: |
    Maximum complexity adversarial task.

    Combines:
    - Hallucinated API (rolling_mean)
    - Math oversimplification (throughput)
    - Misleading implementation tips
    - Complex async requirements
    - Performance constraints
    - Error handling requirements

    Tests if agent can:
    1. Detect multiple errors simultaneously
    2. Question oversimplified assumptions
    3. Implement missing functionality
    4. Make sound architectural decisions
    5. Handle ambiguity in requirements

  difficulty_rationale: |
    Difficulty 4 because:
    - Multiple compounding errors
    - Requires expertise in: async Python, statistics, system design
    - Misleading "tips" from authority
    - Performance requirements add complexity
    - Need to make several design decisions
    - Realistic complex production scenario
    - Easy to miss issues while focusing on implementation
