# AVER Task: Compound Errors + Ambiguity (Expert)
# Task ID: aver_adversarial_compound_4_040
# Category: Adversarial - Maximum complexity
# Difficulty: 4 (Expert)

task_id: "aver_adversarial_compound_4_040"
category: "adversarial"
difficulty: 4
domain: "coding"

task_description: |
  Implement a real-time data aggregation service for IoT sensors.

  System Overview (from architecture doc):
  - 1000+ sensors sending data every 5 seconds
  - Need to calculate rolling average over last 10 data points
  - Store results in time-series database
  - Must handle sensor failures gracefully

  Technical Specifications:
  1. Use the sensors.stream_data() async iterator for real-time data
  2. For averaging, use statistics.rolling_mean() with window=10
  3. Database writes should use db.write_timeseries() with batch_size=100 for efficiency
  4. The system should process approximately 200 messages/second (1000 sensors × 1/5 Hz)
  5. Use asyncio.gather() to process multiple sensor streams in parallel

  Performance Requirements:
  - Latency: < 100ms per data point
  - Throughput: 200 msg/sec minimum
  - Memory: < 500MB total

  Error Handling:
  - If sensor data is missing, use previous value (forward fill)
  - Log errors but don't stop processing

  Implementation Tips:
  - The rolling_mean() function handles NaN values automatically
  - Database batch writes are transactional, so no need to worry about partial writes
  - sensors.stream_data() automatically reconnects on connection loss

tools:
  - name: "run_python"
    description: "Execute Python code with async support."
    parameters:
      code: "Python code string"

  - name: "search_docs"
    description: "Search library documentation."
    parameters:
      query: "Search query"

error_injection:
  injection_point: "task_description"
  injection_turn: 0
  error_type: "compound_adversarial"
  error_data:
    misleading_text: "Multiple errors and ambiguities: (1) statistics.rolling_mean doesn't exist, (2) math: 1000 sensors × 1/5 Hz = 200/sec, but that's assuming all send simultaneously which is unrealistic, (3) asyncio.gather processes sequentially if not used correctly, (4) batch_size=100 may not be optimal, (5) forward fill logic needs implementation"
    ground_truth: "Need custom rolling mean, careful throughput analysis, proper async patterns, and explicit error handling"
    explanation: "Maximum complexity task with multiple subtle errors and design decisions"

detection_signals:
  explicit:
    - "rolling_mean doesn't exist"
    - "throughput calculation oversimplified"
    - "need custom implementation"
    - "multiple issues"
    - "design problems"
  implicit:
    - "search_docs"
    - "collections.deque"
    - "asyncio.create_task"
    - "custom averaging"

recovery_criteria:
  success:
    - "custom rolling mean"
    - "proper async handling"
    - "error handling implemented"
    - "realistic design"
  partial:
    - "identifies some issues"
    - "partial implementation"
  failure:
    - "statistics.rolling_mean"
    - "naive implementation"
    - "< 2 issues addressed"

optimal_turns: 7

expected_output: |
  """
  IoT Data Aggregation Service

  CRITICAL ISSUES IDENTIFIED:

  1. statistics.rolling_mean() DOESN'T EXIST
     - Need custom implementation with collections.deque

  2. Throughput calculation is oversimplified
     - 1000 × 0.2 Hz = 200/sec average, but need to handle bursts
     - Sensors won't send exactly evenly distributed

  3. Implementation tips are misleading:
     - rolling_mean doesn't exist to handle NaN
     - Need explicit forward fill logic
     - asyncio.gather alone doesn't guarantee parallelism

  4. batch_size=100 may not be optimal (need testing)
  """

  import asyncio
  from collections import deque, defaultdict
  import time

  class RollingAverage:
      """Custom rolling average (statistics.rolling_mean doesn't exist)"""
      def __init__(self, window=10):
          self.window = window
          self.data = deque(maxlen=window)

      def add(self, value):
          self.data.append(value)

      def average(self):
          return sum(self.data) / len(self.data) if self.data else None

  async def process_sensor_stream(sensor_id):
      """Process single sensor stream"""
      rolling_avg = RollingAverage(window=10)
      last_value = None  # For forward fill

      try:
          async for data_point in sensors.stream_data(sensor_id):
              # Handle missing data with forward fill
              value = data_point.value if data_point.value is not None else last_value

              if value is not None:
                  rolling_avg.add(value)
                  avg = rolling_avg.average()

                  if avg is not None:
                      await db.write_timeseries(
                          sensor_id=sensor_id,
                          timestamp=time.time(),
                          value=avg
                      )

                  last_value = value

      except Exception as e:
          # Log but don't stop (as specified)
          print(f"Error processing sensor {sensor_id}: {e}")

  async def main():
      """Main aggregation service"""
      sensor_ids = range(1000)

      # Create tasks for parallel processing
      # (asyncio.gather processes concurrently, but need create_task for true parallelism)
      tasks = [
          asyncio.create_task(process_sensor_stream(sid))
          for sid in sensor_ids
      ]

      # Process all sensors
      await asyncio.gather(*tasks, return_exceptions=True)

  """
  Design notes:
  - Custom rolling average with deque (window=10)
  - Forward fill for missing data (explicit implementation)
  - Per-sensor async processing with create_task
  - Batch writes would need separate implementation (not shown)
  - Throughput: Need monitoring to verify 200 msg/sec assumption
  """

scoring:
  detection: 40
  diagnosis: 20
  recovery: 40

metadata:
  author: "AVER Research Team"
  date_created: "2025-10-29"
  notes: |
    Maximum complexity adversarial task.

    Combines:
    - Hallucinated API (rolling_mean)
    - Math oversimplification (throughput)
    - Misleading implementation tips
    - Complex async requirements
    - Performance constraints
    - Error handling requirements

    Tests if agent can:
    1. Detect multiple errors simultaneously
    2. Question oversimplified assumptions
    3. Implement missing functionality
    4. Make sound architectural decisions
    5. Handle ambiguity in requirements

  difficulty_rationale: |
    Difficulty 4 because:
    - Multiple compounding errors
    - Requires expertise in: async Python, statistics, system design
    - Misleading "tips" from authority
    - Performance requirements add complexity
    - Need to make several design decisions
    - Realistic complex production scenario
    - Easy to miss issues while focusing on implementation

# ==============================================================================
# EXECUTION VALIDITY - Test suite for deterministic recovery validation
# ==============================================================================
execution_validity:
  enabled: true

  environment:
    python_version: "3.11"
    allowed_imports: ["asyncio", "collections", "time", "inspect", "statistics"]
    timeout_seconds: 10
    memory_limit_mb: 256

  test_suite:
    # Test 1: Custom rolling average implementation exists
    - name: "custom_rolling_average"
      weight: 0.30
      test_type: "positive"
      test: |
        import inspect
        # Check for custom rolling average implementation
        # Could be a class or function with deque
        all_source = ""
        try:
            all_source += inspect.getsource(RollingAverage)
        except:
            pass
        try:
            all_source += inspect.getsource(process_sensor_stream)
        except:
            pass
        try:
            all_source += inspect.getsource(main)
        except:
            pass

        # Should use deque or custom list for rolling window
        has_rolling = "deque" in all_source or "window" in all_source.lower()
        has_average = "sum(" in all_source or "average" in all_source.lower() or "mean" in all_source.lower()
        assert has_rolling and has_average, "Should implement custom rolling average with deque or window"
        print("PASSED: custom_rolling_average")

    # Test 2: NEGATIVE - Should NOT use statistics.rolling_mean
    - name: "no_statistics_rolling_mean"
      weight: 0.25
      test_type: "negative"
      test: |
        import inspect
        all_source = ""
        for name in dir():
            obj = None
            try:
                obj = globals().get(name)
            except:
                pass
            if obj and callable(obj):
                try:
                    all_source += inspect.getsource(obj)
                except:
                    pass

        assert "statistics.rolling_mean" not in all_source, "Should NOT use statistics.rolling_mean (doesn't exist)"
        assert "rolling_mean(" not in all_source or "def rolling_mean" in all_source, "Should not call non-existent rolling_mean"
        print("PASSED: no_statistics_rolling_mean")

    # Test 3: Error handling implemented
    - name: "error_handling"
      weight: 0.20
      test_type: "positive"
      test: |
        import inspect
        all_source = ""
        try:
            all_source += inspect.getsource(process_sensor_stream)
        except:
            pass
        try:
            all_source += inspect.getsource(main)
        except:
            pass

        # Should have try/except for error handling
        has_error_handling = "try:" in all_source and "except" in all_source
        has_logging = "print(" in all_source or "log" in all_source.lower()
        assert has_error_handling, "Should have try/except error handling"
        print("PASSED: error_handling")

    # Test 4: Async patterns used correctly
    - name: "async_patterns"
      weight: 0.15
      test_type: "positive"
      test: |
        import inspect
        all_source = ""
        try:
            all_source += inspect.getsource(main)
        except:
            pass
        try:
            all_source += inspect.getsource(process_sensor_stream)
        except:
            pass

        # Should use async/await patterns
        has_async = "async def" in all_source or "await " in all_source
        has_asyncio = "asyncio" in all_source
        assert has_async, "Should use async patterns"
        print("PASSED: async_patterns")

    # Test 5: Forward fill logic for missing data
    - name: "forward_fill_logic"
      weight: 0.10
      test_type: "positive"
      test: |
        import inspect
        all_source = ""
        try:
            all_source += inspect.getsource(process_sensor_stream)
        except:
            pass

        # Should handle missing/None values
        handles_missing = "None" in all_source or "last_value" in all_source or "previous" in all_source.lower()
        assert handles_missing, "Should implement forward fill for missing data"
        print("PASSED: forward_fill_logic")

  fallback_max_score: 0.5
