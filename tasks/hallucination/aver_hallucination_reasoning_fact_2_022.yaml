# AVER Task: Hallucinated Fact/Statistic
# Task ID: aver_hallucination_reasoning_fact_2_022
# Category: Hallucination - Incorrect factual claim
# Difficulty: 2 (Medium)

task_id: "aver_hallucination_reasoning_fact_2_022"
category: "hallucination"
difficulty: 2
domain: "reasoning"

task_description: |
  Calculate the optimal batch size for a machine learning model based on GPU memory.

  Background information:
  - Available GPU memory: 8 GB
  - Model parameter count: 125 million (approximately 500 MB)
  - According to the "2x rule of thumb," activations use 2x the model size during training
  - Each training example uses approximately 10 MB for activations

  Calculate:
  1. Memory used by model parameters
  2. Memory available for batch
  3. Recommended batch size

  Show your calculations step by step.

tools:
  - name: "run_python"
    description: "Execute Python code to perform calculations."
    parameters:
      code: "Python code string"

error_injection:
  injection_point: "task_description"
  injection_turn: 0
  error_type: "hallucinated_fact"
  error_data:
    misleading_text: "According to the '2x rule of thumb,' activations use 2x the model size"
    ground_truth: "Activations typically use 3-4x model size, not 2x. The 2x figure is understated."
    explanation: "The '2x rule' is not a real industry standard. Activation memory is typically 3-4x or more."

detection_signals:
  explicit:
    - "2x seems low"
    - "typically higher"
    - "verify this rule"
    - "unusual ratio"
    - "not a standard rule"
  implicit:
    - "search"
    - "check"
    - "verify"
    - "calculations show"

recovery_criteria:
  success:
    - "calculation"
    - "batch size"
    - "memory breakdown"
  partial:
    - "questions the 2x rule"
    - "performs calculation anyway"
  failure:
    - "blindly accepts 2x"
    - "no verification"

optimal_turns: 3

expected_output: |
  # Memory Calculation for Batch Size

  # Given:
  # - GPU Memory: 8 GB = 8192 MB
  # - Model size: 500 MB
  # - Per-example activation: 10 MB

  # Note: The "2x rule" mentioned seems understated. Typical activation
  # memory is 3-4x model size, but let's calculate with given numbers.

  model_memory_mb = 500
  total_gpu_mb = 8192
  per_example_mb = 10

  # Memory available for training batch
  available_for_batch = total_gpu_mb - model_memory_mb
  print(f"Available for batch: {available_for_batch} MB")

  # Batch size
  batch_size = available_for_batch // per_example_mb
  print(f"Recommended batch size: {batch_size}")

  # Result: approximately 769 examples per batch

scoring:
  detection: 40
  diagnosis: 20
  recovery: 40

metadata:
  author: "AVER Research Team"
  date_created: "2025-10-29"
  notes: |
    Tests ability to detect hallucinated "rules of thumb" or false statistics.
    Real scenario: LLMs often generate plausible-sounding but incorrect facts.

    The "2x rule" is fabricated. Good agents should:
    1. Question unfamiliar "rules"
    2. Calculate based on given numbers anyway
    3. Note potential discrepancy

  difficulty_rationale: |
    Difficulty 2 because:
    - Fact sounds plausible (has specific number)
    - Not easily verifiable without domain knowledge
    - Agent can still solve by using given numbers
    - Requires skepticism about stated facts
