# AVER Benchmark - Multi-Model Comparison
# Compare latest frontier LLMs on hallucination detection tasks

[scenario]
name = "aver_multimodel_comparison"
description = "Compare latest GPT, Claude, and Gemini on AVER tasks"
version = "0.1.0"

[config]
tasks_dir = "tasks"
results_dir = "results"
use_llm_judge = false
timeout_seconds = 120

# Model 1: Gemini 3 Pro Preview
[[participants]]
role = "gemini_3_pro"
endpoint = "llm:google/gemini-3-pro-preview"
model = "google/gemini-3-pro-preview"
description = "Gemini 3 Pro Preview via OpenRouter"
enabled = true

# Model 2: Claude Sonnet 4.5
[[participants]]
role = "claude_sonnet_45"
endpoint = "llm:anthropic/claude-sonnet-4.5"
model = "anthropic/claude-sonnet-4.5"
description = "Claude Sonnet 4.5 via OpenRouter"
enabled = true

# Model 3: GPT-5.1 Codex
[[participants]]
role = "gpt51_codex"
endpoint = "llm:openai/gpt-5.1-codex"
model = "openai/gpt-5.1-codex"
description = "GPT-5.1 Codex via OpenRouter"
enabled = true

[task_selection]
# Use specific tasks so all models get the SAME tasks for fair comparison
task_ids = ["aver_hallucination_code_api_2_001", "aver_hallucination_code_api_2_003", "aver_negative_yaml_001"]
category = ""
difficulty = 0
num_tasks = 3
