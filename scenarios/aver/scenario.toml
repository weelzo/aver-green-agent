# AVER Benchmark Scenario Configuration
# AgentBeats / A2A Protocol Setup

[scenario]
name = "aver_benchmark"
description = "Agent Verification & Error Recovery Benchmark"
version = "0.1.0"

[green_agent]
# AVER Green Agent configuration
endpoint = "http://127.0.0.1:9000"
model = "gemini-2.0-flash"  # or any LLM for orchestration
role = "aver_orchestrator"

[config]
# AVER benchmark configuration
tasks_dir = "tasks"
results_dir = "results"
num_tasks = 5
categories = ["hallucination", "validation", "tool_misuse", "context_loss", "adversarial"]
difficulty_levels = [1, 2, 3, 4]

# Evaluation settings
use_llm_judge = false
timeout_seconds = 120
max_turns_per_task = 10

# Sandbox settings
use_docker_sandbox = false  # Set to true for isolated execution
sandbox_memory_limit = "512m"

# Reporting
save_results = true
generate_report = true
verbose = true

# Universal mock agent for demo (enabled by default)
[[participants]]
role = "mock_universal_agent"
endpoint = "mock"
model = "mock-gpt-4"
description = "Universal mock agent demonstrating varied behaviors"
enabled = true

# Real agent to test (disabled - enable when you have an A2A server running)
[[participants]]
role = "test_agent"
endpoint = "http://127.0.0.1:8001"
model = "gpt-4"
description = "Agent under test for AVER benchmark"
enabled = false

# LLM agents via OpenRouter API (no A2A server required!)
# Use "llm:<model>" format - see available models at openrouter.ai/models
[[participants]]
role = "gpt4_turbo_agent"
endpoint = "llm:gpt-4-turbo"
model = "gpt-4-turbo"
description = "Real GPT-4 Turbo via OpenRouter"
enabled = false  # Enable to test with real LLM (costs money!)

[[participants]]
role = "claude_sonnet_agent"
endpoint = "llm:claude-3.5-sonnet"
model = "claude-3.5-sonnet"
description = "Real Claude 3.5 Sonnet via OpenRouter"
enabled = false

[[participants]]
role = "gemini_flash_agent"
endpoint = "llm:gemini-2.0-flash"
model = "gemini-2.0-flash"
description = "Real Gemini 2.0 Flash via OpenRouter (free tier available)"
enabled = false

# Legacy: A2A protocol agents (requires running A2A server)
[[participants]]
role = "a2a_test_agent"
endpoint = "http://127.0.0.1:8001"
model = "gpt-4"
description = "Agent via A2A protocol (requires running server)"
enabled = false

# Task filters
[task_selection]
# Select specific task or use filters
task_id = ""  # Leave empty for random selection
category = ""  # Empty = all categories
difficulty = 0  # 0 = all difficulties
num_tasks = 40  # Full benchmark demo

# Advanced settings
[advanced]
# Error injection
error_injection_enabled = true
inject_at_runtime = false

# Trace collection
collect_full_traces = true
include_tool_outputs = true
save_traces = true

# Evaluation
detection_weight = 40
diagnosis_weight = 20
recovery_weight = 40

# Retry settings
max_retries = 3
retry_delay_seconds = 2

# Logging
log_level = "INFO"
log_file = "logs/aver_benchmark.log"
